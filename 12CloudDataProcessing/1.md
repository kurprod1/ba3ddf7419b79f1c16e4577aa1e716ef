# Big Data in Cloud: Intro

Sebelumnya sudah dibahas mengenai ekosistem hadoop. Di bagian ini akan dibahas problem yang ada di Hadoop On Premise Infrastruktur. Semakin berjalannya waktu maka perkembangan jumlah data semakin besar sehingga butuh untuk menambah harddisk di server on premise. Sedangkan untuk menambahkan hard disk tentu tidak mudah, membutuhkan proses yang lama dan harganya yang sangat mahal. Berikut adalah beberapa problem yang ada di Hadoop On Premise Infrastruktur :

- Not Elasticity
  
    Elasticity artinya mampu menambah atau mengurangi sumber sesuai kebutuhan secara cepat. Di Hadoop On premise, misal untuk menambah slot harddisk membutuhkan waktu yang lama.

- Cost Very Expensive
  
    Di Hadoop On premise, misal di cloudera membutuhkan minimal 5 node(2 master node dan 3 worker node) dimana masing-masing node membutuhkan spesifikasi komputer tinggi minimal RAM 32 GB, Hard disk SSD sehingga membutuhkan biaya yang sangat mahal di muka untuk membangun Hadoop On Premise.

- Availability Not Guaranteed
  
    Di hadoop on premise, tingkat ketersediaan server tergantung dari kualitas hardware sehingga tidak bisa menjamin ketersediaan server selalu menyala.

- Instalasi rumit
  
    Di hadoop on premise, instalasi hadoop cukup rumit dan membutuhkan skill di administrasi linux sehingga sulit untuk pemula di linux.

Jadi kesimpulan dari problem yang ada di Hadoop on premise adalah Not Elasticity, cost very expensive dan availability not guaranteed.

# **Kenapa harus di Cloud ?**

Sebelumnya sudah dibahas problem hadoop di on premise. Untuk menangani problem tersebut solusinya adalah harus menggunakan cloud. Solusi yang ditawarkan oleh cloud yaitu :

- Elasticity

Di Cloud, misal untuk menambah slot harddisk membutuhkan waktu yang cepat.

Tipe harddisk apakah HDD atau SSD, lalu untuk pengolahan komputasi seperti CPU, GPU atau TPU. CPU(Central Processing Unit) adalah pengolahan komputasi seperti di komputer biasa yang membutuhkan processor untuk melakukan pengolahan komputasi di komputer. GPU(Graphic Processing Unit) adalah pengolahan komputasi yang membutuhkan VGA Card External. TPU adalah pengolahan komputasi yang disediakan oleh tensorflow khusus untuk pengolahan data(gambar, video) yang menggunakan deep learning yang berat

- Cost not expensive(based on your usage)

Untuk biaya yang dikeluarkan di cloud sendiri cenderung murah tergantung dari pemakaian masing2. Misalnya jika sudah selesai digunakan bisa dimatikan servicenya di cloud karena sistem cost di semua penyedia cloud adalah *pay per use* yang artinya bayar apa yang kalian gunakan saja.

- Guaranteed Availability

Di semua cloud untuk availability di jamin 99.9 %. Misal untuk Google Cloud Storage di jamin 99.9 % availability oleh Google Cloud.

- Easy Install

Install Hadoop di cloud mudah sekali, tinggal tentukan jenis machine nya(CPU, RAM), hard disk

Beberapa contoh alternatif komponen Hadoop yang lebih efisien dan murah di Cloud

Contoh-contoh alternatif komponen Hadoop yang lebih efisien dan murah di Google Cloud yaitu :

- **Google Cloud Storage** : alternatif pengganti HDFS. selain high availability juga bisa menyimpan data dalam waktu yang lama dengan harga yang lebih murah.
- **Cloud DataFlow**: alternatif pengganti Apache Pig. Dataflow support Batch dan streaming juga tidak banyak pengaturan untuk menggunakan nya(100% code).
- **Bigquery**: alternatif pengganti Apache Hive dan Apache Mahout. Bigquery support untuk menyimpan data yang jumlah nya sangat besar dan juga menjalankan query sangat cepat dalam hitungan detik. Banyak fungsi yang ada di Bigquery salah satu nya bisa membuat model machine learning menggunakan query sql.

## **Contoh beberapa perusahaan penyedia Cloud besar yang ada**

Contoh Perusahaan cloud besar yang terkenal yaitu :

- Amazon Web Service
- Google Cloud Platform
- Microsoft Azure
- Alibaba Cloud
- Digital Ocean