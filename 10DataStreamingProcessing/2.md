# Data Streaming Tools

# Mind Map

Dalam mempelajari tools yang dapat digunakan untuk keperluan streaming, kita dapat mengikuti mindmap dibawah.

![streaming Mind Map.jpg](Data%20Streaming%20Tools%205f2c45fbf19344c5bc1ee27cadd32de2/streaming_Mind_Map.jpg)

# Apache Kafka

Dalam Big Data, digunakan volume data yang sangat besar. Mengenai data, ada dua tantangan utama. Tantangan pertama adalah bagaimana mengumpulkan data dalam jumlah besar dan tantangan kedua adalah menganalisis data yang terkumpul. Untuk mengatasi tantangan tersebut, kita harus membutuhkan messaging system.

[https://lh3.googleusercontent.com/NRSO4XzAyX2S--ZupQRxHd8C8dyp8Qx1AtE-dobgyRvtoV7OfMUd-gNtQXLfkYhLzXQqaVCuNLlumjlRCe_7-cMgYcv2ZbwL6aywXQaJ7AtSTs46fYpEKFxY8xZkDwFzPv-EiJ5CUY69ap_zc3Pv2Q](https://lh3.googleusercontent.com/NRSO4XzAyX2S--ZupQRxHd8C8dyp8Qx1AtE-dobgyRvtoV7OfMUd-gNtQXLfkYhLzXQqaVCuNLlumjlRCe_7-cMgYcv2ZbwL6aywXQaJ7AtSTs46fYpEKFxY8xZkDwFzPv-EiJ5CUY69ap_zc3Pv2Q)

Apache Kafka adalah messaging system publish-subscribe terdistribusi yang dapat menangani volume data yang tinggi dan memungkinkan untuk meneruskan message dari satu titik akhir ke titik akhir lainnya. Kafka cocok untuk konsumsi message offline dan online. Message Kafka disimpan di disk dan direplikasi dalam cluster untuk mencegah kehilangan data. Kafka dibangun di atas layanan sinkronisasi ZooKeeper. Ini terintegrasi dengan sangat baik dengan Apache Storm dan Spark untuk analisis data streaming realtime.

## **Apa itu Messaging System?**

Messaging System bertanggung jawab untuk mentransfer data dari satu aplikasi ke aplikasi lain, sehingga aplikasi dapat fokus pada data, tetapi tidak khawatir tentang cara membagikannya. message terdistribusi didasarkan pada konsep queue message yang andal. Message diantrekan secara asinkron antara aplikasi klien dan sistem messaging. Tersedia dua jenis pola message yaitu:

- point to point
- publish-subscribe (pub-sub)

## ***Sistem Pesan point to point***

Dalam sistem point-to-point, pesan tetap ada dalam antrian. Satu atau lebih konsumen dapat mengkonsumsi message dalam antrian, tetapi pesan tertentu dapat dikonsumsi oleh maksimal satu konsumen saja. Setelah konsumen membaca message dalam antrian, message menghilang dari antrian itu.

[https://lh6.googleusercontent.com/-isAsVc70YqZdUV9Gzie-Qt-jN211kGSbq5atpuVM5R1fj9T0MhsYmpwWdqaG5SkKIJiZQhtApGrgYajOv_pH5z5zqUv_BbFTmVqVsTQjcA__wTt6-THCg1WDRD5_DiPoRyZoKuaWdZLe9dADpZezQ](https://lh6.googleusercontent.com/-isAsVc70YqZdUV9Gzie-Qt-jN211kGSbq5atpuVM5R1fj9T0MhsYmpwWdqaG5SkKIJiZQhtApGrgYajOv_pH5z5zqUv_BbFTmVqVsTQjcA__wTt6-THCg1WDRD5_DiPoRyZoKuaWdZLe9dADpZezQ)

## ***Sistem Pesan Publish-Subscribe***

Dalam sistem publish-subscribe, message tetap ada dalam sebuah topik. Tidak seperti sistem point-to-point, konsumen dapat berlangganan satu atau lebih topik dan mengkonsumsi semua message dalam topik tersebut. Dalam sistem Publish-Subscribe, produsen message disebut publisher dan konsumen pesan disebut subsciber. Contoh nyata adalah TV, yang menyiarkan berbagai saluran seperti olahraga, film, musik, dll., dan siapa pun dapat berlangganan saluran mereka sendiri dan mendapatkannya kapan pun saluran berlangganan mereka tersedia.

[https://lh4.googleusercontent.com/EFo7xXi6NK5HqCeKq9u-i3aZJMyVbVNS7I5AsKn0tc2vJ-565n63-uVcGScFBhT-gAQeE6eSmr8SzXsmgnthGYKug_IVv-hz1sQB1Jw5KRDIu03o0tMCOHvQtwFoi29Gam1M8FwLQ8tMwpwqfMI2TQ](https://lh4.googleusercontent.com/EFo7xXi6NK5HqCeKq9u-i3aZJMyVbVNS7I5AsKn0tc2vJ-565n63-uVcGScFBhT-gAQeE6eSmr8SzXsmgnthGYKug_IVv-hz1sQB1Jw5KRDIu03o0tMCOHvQtwFoi29Gam1M8FwLQ8tMwpwqfMI2TQ)

## **Kapan Menggunakan Kafka ?**

Kafka adalah platform terpadu untuk menangani semua feed data realtime. Kafka mendukung pengiriman message dengan latensi rendah dan memberikan jaminan fault tolerant dengan adanya kegagalan mesin. Ini memiliki kemampuan untuk menangani sejumlah besar konsumen yang beragam. Kafka sangat cepat, melakukan 2 juta penulisan/detik. Kafka menyimpan semua data ke disk, yang pada dasarnya berarti bahwa semua penulisan masuk ke cache OS (RAM). Ini membuatnya sangat efisien untuk mentransfer data dari cache halaman ke soket jaringan.

## Dokumentasi

[Apache Kafka](https://kafka.apache.org/documentation/)

## Study Case

[Your Complete Guide to Apache KafkaÂ® Streams](https://www.instaclustr.com/blog/kafka-streams-guide/)

[https://www.youtube.com/watch?v=Z8_O0wEIafw](https://www.youtube.com/watch?v=Z8_O0wEIafw)

# Spark Streaming

Spark Streaming adalah tambahan dari API Spark Core yang memungkinkan pemrosesan live data stream yang scalable, High Throughput, dan fault tolerance. Data dapat diolah dari berbagai sumber seperti Kafka, Kinesis, atau TCP socket, dan dapat diproses menggunakan algoritma kompleks yang diekspresikan dengan fungsi tingkat tinggi seperti map, reduce, join, dan window. Akhirnya, data yang diproses dapat didorong ke sistem file, database, dan dashboard langsung. Bahkan, kalian dapat menerapkan machine learning dan algoritma pemrosesan grafik Spark pada data streaming.

[https://lh4.googleusercontent.com/LHoBeir_qLEEf1xIXGKVS8aLY-nQxvXGvyHm8DdmVcLw9T-GQKA1J0Da7-P8wcaYgcxTxFyG25UhSJbQALX03o6wPQOvkIuAawFzURa5zua5RLV2jHpsxhPLrhLPr3goCnKwo0aBn5Wjg0O3mG2BGQ](https://lh4.googleusercontent.com/LHoBeir_qLEEf1xIXGKVS8aLY-nQxvXGvyHm8DdmVcLw9T-GQKA1J0Da7-P8wcaYgcxTxFyG25UhSJbQALX03o6wPQOvkIuAawFzURa5zua5RLV2jHpsxhPLrhLPr3goCnKwo0aBn5Wjg0O3mG2BGQ)

Secara internal, Apache Spark Streaming berfungsi seperti pada gambar dibawah. Spark Streaming menerima streaming data input langsung dan membagi data menjadi batch, yang kemudian diproses oleh mesin Spark untuk menghasilkan streaming hasil akhir dalam batch.

[https://lh3.googleusercontent.com/AMiqVIhL_y161ikPSveoEXGyGC2-dWPOfS4634v6aWSE1-mUCYIjXkuYykZ7ZtbB_7tLYsolhKJeWgjRv-XyB8nbAzJ9y7tfEOiKhenbV-q1nssdQmNwRIrjD0N7uM1cdGl2ASl0sEl_fHEFgat3vg](https://lh3.googleusercontent.com/AMiqVIhL_y161ikPSveoEXGyGC2-dWPOfS4634v6aWSE1-mUCYIjXkuYykZ7ZtbB_7tLYsolhKJeWgjRv-XyB8nbAzJ9y7tfEOiKhenbV-q1nssdQmNwRIrjD0N7uM1cdGl2ASl0sEl_fHEFgat3vg)

Spark Streaming menyediakan abstraksi tingkat tinggi yang disebut Discretized Stream atau DStream, yang mewakili data streaming berkelanjutan. DStreams dapat dibuat dari input data stream dari sumber-sumber seperti Kafka, dan Kinesis, atau dengan menerapkan operasi tingkat tinggi pada DStreams lainnya. Secara internal, DStream direpresentasikan sebagai urutan RDD.

## **DStream**

Discretized Stream atau DStream adalah abstraksi dasar yang disediakan oleh Spark Streaming. DStream mewakili stream data yang berkelanjutan, baik stream data input yang diterima dari sumber, atau stream data yang diproses yang dihasilkan dengan mengubah stream input. Secara internal, DStream diwakili oleh serangkaian RDD yang berkelanjutan, yang merupakan abstraksi Spark dari dataset terdistribusi yang tidak dapat diubah. Setiap RDD dalam DStream berisi data dari interval tertentu, seperti yang ditunjukkan pada gambar berikut.

[https://lh3.googleusercontent.com/xxlch5r2ANX9qSl9wYG2z_hjMKdaaBBIac3L4Zng8Q0GfSaa2iIQuLQDOJcq23mMhZeJTiWA8LE44S2N__RdeYVrxEpFEmQVA4xhf3f4DBll7HKmth_QPbVZ3Mi7WaINtNlyHIQHCdXtnsAyi_5oNA](https://lh3.googleusercontent.com/xxlch5r2ANX9qSl9wYG2z_hjMKdaaBBIac3L4Zng8Q0GfSaa2iIQuLQDOJcq23mMhZeJTiWA8LE44S2N__RdeYVrxEpFEmQVA4xhf3f4DBll7HKmth_QPbVZ3Mi7WaINtNlyHIQHCdXtnsAyi_5oNA)

Setiap operasi yang diterapkan pada DStream berarti operasi pada RDD yang mendasarinya. Misalnya, dalam contoh sebelumnya dari mengubah stream data baris ke kata-kata, operasi flatMap diterapkan pada setiap RDD di baris DStream untuk menghasilkan RDD dari kata-kata DStream. Ini ditunjukkan pada gambar berikut.

[https://lh5.googleusercontent.com/gP4oWtP_cgWazCB2s7TkzQTgIOGh-CnmmEyd9gsnqvw347AfaprvP5wLVHmB6fv3lNxeIX578uDTHkhR9gfbZBvCeSGos7oNoN83CjPRPvTk5AdvuuBao6-gX4riHh3-beXoRhvjnwF5KqyMuBsfig](https://lh5.googleusercontent.com/gP4oWtP_cgWazCB2s7TkzQTgIOGh-CnmmEyd9gsnqvw347AfaprvP5wLVHmB6fv3lNxeIX578uDTHkhR9gfbZBvCeSGos7oNoN83CjPRPvTk5AdvuuBao6-gX4riHh3-beXoRhvjnwF5KqyMuBsfig)

Transformasi RDD yang mendasarinya dihitung oleh mesin Spark. Dengan arsitektur micro-batch, Spark Streaming dapat menerima data dari berbagai sumber dan membaginya menjadi kelompok-kelompok kecil. Kumpulan inputbaru dibuat pada interval waktu reguler (contoh : parameter interval batch).

Setelah mendapatkan data dari sumber streaming dan menyimpannya dalam memori Spark, Spark core digunakan sebagai mesin pengolah batch untuk memproses setiap batch data. Karena model perhitungan di belakang Spark Streaming didasarkan pada Dstreams, input data stream didiskritisasi menjadi batch dan diwakili dalam DStream yang disimpan sebagai urutan RDD dalam memori Spark. Kemudian, perhitungan streaming dijalankan dengan menghasilkan pekerjaan Spark untuk memproses RDD tersebut. Ini menghasilkan RDD lain yang mewakili keluaran program atau negara perantara. Hasil pemrosesan tersebut juga dapat didorong ke sistem eksternal dalam batch.

## Dokumentasi

[Spark Streaming - Spark 3.3.0 Documentation](https://spark.apache.org/docs/latest/streaming-programming-guide.html)

# Kafka + Spark Integration

Kita bisa mengkombinasikan kedua tools diatas agar menjadi sebuah pipeline yang powerful. Berikut adalah panduan bagaimana Kafka bisa diintegrasikan dengan Spark streaming.

[Structured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)