# ETL using Apache Spark

Spark adalah kerangka kerja untuk analisa dan memproses data dalam skala besar. Spark memiliki berapa komponen pendukung seperti Graphx, Spark SQL, Spark Streaming dan MLlib. Ini berjalan cepat 100x lebih cepat daripada Hadoop MapReduce tradisional karena operasi dalam memori, menawarkan objek data yang kuat, terdistribusi, toleran terhadap kesalahan dan terintegrasi dengan indah dengan Machine Learning dan analisis grafik melalui paket tambahan seperti Mlib dan GraphX.

[https://lh4.googleusercontent.com/W0WmTlSS3Ju910pn0HhIqqygXnTD3IljXrWyRo9PsVBdwa8IruQKATG-Ev1PJvzlSxw0imHUItJ3X7ItWqW63rQbeCQ5od3FVETLxPk7qpgt-ojHpWZe_Mx9nrLGIaC-QkzouViXs09zpyqbeUX9tQ](https://lh4.googleusercontent.com/W0WmTlSS3Ju910pn0HhIqqygXnTD3IljXrWyRo9PsVBdwa8IruQKATG-Ev1PJvzlSxw0imHUItJ3X7ItWqW63rQbeCQ5od3FVETLxPk7qpgt-ojHpWZe_Mx9nrLGIaC-QkzouViXs09zpyqbeUX9tQ)

Spark menyediakan integrasi Python yang luar biasa, yang disebut PySpark , yang memungkinkan pemrogram Python untuk berinteraksi dengan kerangka kerja Spark dan mempelajari cara memanipulasi data dalam skala besar dan bekerja dengan objek dan algoritme melalui sistem file terdistribusi.

Salah satu kegunaan Pyspark ialah untuk menganalisa, memanipulasi data dalam skala besar. Menggunakan metode ETL, pyspark sangat berguna terutama untuk data engineer untuk mengolah data dengan skala sangat besar.

Kita bisa menginstall library PySpark ini menggunakan perintah pip berikut

```bash
pip install pyspark
```

Dengan begitu, program python dapat berkomunikasi dengan framework Spark.

Untuk memberikan gambaran dalam belajar, ada baiknya kalian membaca artikel berikut.

[Create your first ETL Pipeline in Apache Spark and Python](https://towardsdatascience.com/create-your-first-etl-pipeline-in-apache-spark-and-python-ec3d12e2c169)

Jangan hanya terpaku ke studi kasus diatas, ya. Carilah sumber-sumber belajar lainnya, jangan malu berkonsultasi kepada instruktur, dan yang terpenting adalah kerjakan project dan perbanyak hands-on!