# Spark Operation

Pada modul sebelumnya sempat disinggung mengenai Spark ini, namun hanya sedikit informasi mengenai Spark tersebut. Kali ini kita akan membahas lebih lanjut mengenai environment satu ini.

Untuk mempermudah peta belajar kita, berikut adalah mind map yang bisa kami berikan.

![spark learning Mind Map.jpg](Spark%20Operation%201c21fb766f51421f98759dc47d65357b/spark_learning_Mind_Map.jpg)

Spark Core berisi fungsi dasar Spark, termasuk komponen untuk scheduling, manajemen memori, fault tolerance, interaksi dengan sistem penyimpanan, dan masih banyak lagi. Spark Core juga merupakan home bagi API yang mendefinisikan kumpulan data terdistribusi atau yang dapat disebut dengan Resilient Distributed Dataset (RDD), yang merupakan abstraksi pemrograman utama Spark. RDD mewakili kumpulan item yang didistribusikan di banyak node komputasi yang dapat dimanipulasi secara paralel. Spark Core menyediakan banyak API untuk membangun dan memanipulasi koleksi ini. Spark Core sendiri, dapat dijalankan menggunakan 4 bahasa, yaitu Java, Python, R, dan Scala.

![Untitled](Spark%20Operation%201c21fb766f51421f98759dc47d65357b/Untitled.png)

Untuk memulai menggunakan PySpark dan turunan-turunannya, kita bisa coba melihat video berikut.

[![https://www.youtube.com/watch?v=_C8kWso4ne4](https://img.youtube.com/vi/_C8kWso4ne4/0.jpg)](https://www.youtube.com/watch?v=_C8kWso4ne4)