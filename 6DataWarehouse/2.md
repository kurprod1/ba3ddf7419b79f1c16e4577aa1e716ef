# Apache Hive Introduction

Apache Hive merupakan project data warehouse dan analytics yang dibangun di atas platform Apache Hadoop. 

![Untitled](Apache%20Hive%20Introduction%20a7e842a28a944e4fb69d4da9777847d9/Untitled.png)

Tools ini awalnya dikembangkan oleh developer Facebook yang menilai cara akses/query data yang disimpan di platform Hadoop terlalu susah karena masih harus menggunakan konsep Map and Reduce yang implementasinya menggunakan programming Java. Untuk itu sebagai solusinya mereka menciptakan sebuah interface yang memungkinkan untuk melakukan retrieval dan manipulasi data menggunakan bahasa yang serupa dengan syntax SQL.

# Arsitektur Apache Hive

Banyak orang yang salah memahami akan Apache Hive, banyak yang menganggap Apache Hive adalah sebuah engine database. Padahal faktanya data yang di query melalui Apache Hive ini disimpan secara terdistribusi dengan rapi pada filesystem HDFS datanode Hadoop, seperti yang dapat dilihat dari gambar di bawah.

![Source: [https://cwiki.apache.org/confluence/display/hive/design](https://cwiki.apache.org/confluence/display/hive/design)](Apache%20Hive%20Introduction%20a7e842a28a944e4fb69d4da9777847d9/Untitled%201.png)

Source: [https://cwiki.apache.org/confluence/display/hive/design](https://cwiki.apache.org/confluence/display/hive/design)

Apache Hive menyederhanakan cara akses ke data yang disimpan pada HDFS tersebut dengan membuat compiler query yang menyerupai SQL, disebut dengan HiveQL, untuk kemudian diterjemahkan ke dalam Map Reduce Hadoop. Selain itu, Hive juga menyimpan struktur data, seperti halnya kolom pada tabel RDBMS dalam modul Hive Metastore.

**Read more:** 

[Design](https://cwiki.apache.org/confluence/display/hive/design)

[Hive Architecture - Javatpoint](https://www.javatpoint.com/hive-architecture)

# **Bagaimana arsitektur Hive, Spark, dan Hadoop bisa bersama?**

Pada modul modul sebelumnya, kita sudah memahami ekosistem dari Hadoop itu sendiri. Dimana hadoop ecosystem terdiri dari banyak komponen dari HDFS, MapReduce, YARN, dan common.

Namun pada tahun 2008, Hadoop berkembang dan memiliki banyak komponen yang saling membutuhkan satu sama lain. Ini sebabnya hadoop dipilih untuk mengelola big data karena kemampuannya untuk memproses, mengelola resource dan menyimpan data yang sangat besar.

## **Kapan menggunakan Hive kapan menggunakan Spark**

Hive dan Spark adalah dua produk yang sangat populer dan sukses untuk memproses kumpulan data skala besar. Dengan kata lain, mereka melakukan analitik big data. Pada kali ini, kita akan membandingkan kemampuan mereka akan menggambarkan berbagai masalah pemrosesan data kompleks yang dapat diatasi oleh kedua produk ini.

## **Mengapa Hive?**

Alasan inti memilih Hive adalah karena ini adalah antarmuka SQL yang beroperasi di Hadoop. Selain itu, ini mengurangi kompleksitas framework MapReduce. Hive membantu melakukan analisis data skala besar untuk bisnis di HDFS, menjadikannya database yang dapat diskalakan secara horizontal. Antarmuka SQL-nya, HiveQL, memudahkan pengembang yang memiliki latar belakang RDBMS untuk membangun dan mengembangkan kerangka kerja tipe data warehousing yang berkinerja lebih cepat dan skalabel.

## **Fitur dan Kemampuan Hive**

Hive hadir dengan fitur dan kapabilitas tingkat perusahaan yang dapat membantu organisasi membangun solusi data warehousing yang efisien dan canggih.

Beberapa fitur tersebut antara lain:

- Hive menggunakan Hadoop sebagai mesin penyimpanannya dan hanya berjalan pada HDFS.
- Ini dibuat khusus untuk operasi data warehousing..
- HiveQL adalah mesin SQL yang membantu membangun kueri SQL kompleks untuk operasi jenis data warehousing. Hive dapat diintegrasikan dengan database terdistribusi lain seperti HBase dan database NoSQL seperti Cassandra

## **Mengapa Spark?**

Kekuatan Spark Core adalah kemampuannya untuk melakukan in-memory analytic yang kompleks dan data streaming berukuran hingga petabyte, membuatnya lebih efisien dan lebih cepat daripada **Hadoop MapReduce**. Spark dapat memproses data dari penyimpanan data apa pun yang berjalan di Hadoop dan melakukan in-memory analytic kompleks dan secara paralel. Kemampuan ini mengurangi tabrakan Disk I/O dan Network , membuatnya sepuluh kali atau bahkan seratus kali lebih cepat. Selain itu, kerangka kerja analisis data di Spark dapat dibuat menggunakan Java, Scala, Python, R, atau bahkan SQL.