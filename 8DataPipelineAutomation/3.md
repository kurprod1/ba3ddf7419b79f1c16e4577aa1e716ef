# Apache Airflow Introduction

Tools untuk workflow otomasi data pipeline banyak baik yang open source maupun yang berbayar. Di course ini kita menggunakan tools Apache Airflow karena bersifat gratis, open source dan juga populer digunakan oleh banyak perusahaan perusahaan besar. Dan juga selain itu karena dokumentasi yang rapi dan cukup banyak terintegrasi dengan cloud provider salah satu nya dengan Google Cloud.

# **Apa itu Airflow ?**

Sebelum mempelajari Airflow maka kita harus tahu dulu apa itu workflow. Workflow Management Systems (WFMS) adalah suatu cara untuk mengatur tugas-tugas yang harus dikerjakan untuk mencapai tujuan/tahapan tertentu agar tugas-tugas tersebut tidak repetitive.

![https://lh4.googleusercontent.com/TfwWlxMMKI0Lbo-sx5mcfXYCQWvy_kUvJLOayaKCyTJkc9m1ecuVQLAZUwNUBrCKL4as4HDDCNz7Lk-bw7V3-O0GYOHId5OcG8ZEiO7WfCgFeqUxcEwdLc4mkLglgN94Ak1InqIREfFcH1gK3A](https://lh4.googleusercontent.com/TfwWlxMMKI0Lbo-sx5mcfXYCQWvy_kUvJLOayaKCyTJkc9m1ecuVQLAZUwNUBrCKL4as4HDDCNz7Lk-bw7V3-O0GYOHId5OcG8ZEiO7WfCgFeqUxcEwdLc4mkLglgN94Ak1InqIREfFcH1gK3A)

Lebih detailnya **Workflow** adalah serangkaian tugas-tugas yang harus dikerjakan entah secara berurutan maupun tidak, biasanya tugas-tugas tersebut memiliki ketergantungan satu sama lain. Management adalah cara mengatur tugas-tugas diatas. Sedangkan System adalah serangkaian komponen yang saling berkaitan demi mencapai tujuan tertentu. Apa tujuannya? jelas untuk menyederhanakan proses yang berulang-ulang.

Kembali lagi ke [Apache Airflow, **Airflow**](https://airflow.apache.org/) adalah tool open source untuk membuat, scheduling, dan monitor workflow secara terprogram. Ini adalah salah satu platform yang dapat diandalkan oleh Data Engineer untuk mengatur work flow atau pipelines. Kita dapat dengan mudah memvisualisasikan dependencies, progress, log, code, trigger task, dan status pipeline data dari sebuah workflow.

Dengan Airflow, pengguna dapat membuat workflow sebagai Directed Acyclic Graphs (DAGs) yang akan kita bahas sebentar lagi. Interface Airflow yang berupa dashboard memudahkan untuk memvisualisasikan pipelines yang berjalan dalam production, memantau progress, dan memecahkan masalah saat diperlukan.

Selain itu, Airflow juga dapat terintegrasi dengan beberapa sumber data dan dapat mengirim peringatan melalui email atau Slack ketika task selesai atau gagal. Airflow terdistribusi, scalable, dan flexible, sehingga cocok untuk menangani permasalahan Big Data yang kompleks, terutama dibidang bisnis yang.

# How does Airflow work?

Berikut ini adalah arsitektur Apache Airflow secara umum, yang menunjukkan bahwa Apache Airflow memiliki beberapa komponen diantaranya: **Worker**, **Scheduler**, **Web UI (Dashboard)**, **Web Server**, **Database**, dst dalam menjalankan tugasnya dan untuk menggerakkan workflow yang kita buat.

![*Arsitektur Apache Airflow*](https://lh6.googleusercontent.com/5dId6IM-B45YtvdsugfOLitS3HgQUhQUy6gQw0qSvv12PwXosyMSwz4YcxYsBn5_W43gXyQKlFo_-eRx3ttK1SHRB6ybdU142GQTEFDtLX45hTAbYD50tY6w4lA0WbKt8X214QNM90z4RStpaw)

*Arsitektur Apache Airflow*

Keterangan :

- Worker adalah komponen yang benar-benar menjalankan logika Task, dan worker ini ditentukan oleh Executor yang digunakan, dengan kata lain, Worker lah yang melakukan tugas yang diberikan oleh Operator (Task). Jika dianalogikan sama seperti kita memesan makanan menggunakan layanan GO-FOOD milik GO-JEK, misal kita ingin makan bakso (Task) karena malas keluar rumah kita menggunakan GO-FOOD (Executor) nah si driver yang bertugas untuk membeli bakso adalah Worker, karena layanan GO-FOOD lah yang menentukan siapa driver yang mendapatkan giliran menerima order.
- Scheduler (penjadwalan) adalah otak di balik pengaturan workflow di Airflow atau jika dianalogikan, Scheduler adalah “petugas” yang bertanggung jawab dalam memantau **semua DAG beserta Tasks** yang ada, dan memicu (men-*trigger*) **semua** *Task Instances* yang dependensinya telah terpenuhi, scheduler ini juga yang memastikan DAGs yang ada di dalam *DagBag* tetap tersinkronisasi dengan Airflow, makanya setiap kali kita menambahkan satu berkas Python berisi DAG kedalam DagBag Airflow akan segera mengetahuinya dan menampilkannya di Web Dashboard (selama scheduler dijalankan).
- Executor adalah *message queuing* yang terikat erat dengan Scheduler dan menentukan proses worker yang benar-benar mengeksekusi setiap Task yang dijadwalkan. Ada berbagai macam jenis Executor, yang masing-masing menggunakan class Executor khusus. Sebagai contoh, LocalExecutor mengeksekusi Task secara paralel yang berjalan pada mesin yang sama dengan Scheduler. Executor lainnya seperti CeleryExecutor mengeksekusi Task menggunakan worker yang ada pada “sekelompok mesin” worker yang terpisah ini biasa disebut sebagai *distributed worker* atau dengan kata lain worker yang terdistribusi.
- Metadata Database adalah database untuk menyimpan metadata Airflow yang terdiri dari setting config, Scheduler, dst.

Meskipun pada kenyataannya Airflow merupakan tool pemrosesan sebuah workflow secara scheduled (automated), jangan menyalah artikan bahwa Airflow akan sangat baik berjalan untuk Streaming Data (Apache Spark Streaming, Storm, dll) karena Airflow didisain khusus untuk memproses workflow, bukan sebagai data streaming.

Lalu bagaimana contoh penggunaannya pada Big Data? Perhatikan gambar dibawah berikut.

[https://lh6.googleusercontent.com/IvaYaKa0vpcPO9mzH7zSMiC4tebZwr4SxQY9C4GTk-9gQsloEFzkQlahh7Tokx9fKejRqsNiUUQI5pV86TJhJdU7ceEAf9ry6n2MI19niAIHi7fTnckyMdqbxv6kEPq9VlGmSMAYyRHztLd_ng](https://lh6.googleusercontent.com/IvaYaKa0vpcPO9mzH7zSMiC4tebZwr4SxQY9C4GTk-9gQsloEFzkQlahh7Tokx9fKejRqsNiUUQI5pV86TJhJdU7ceEAf9ry6n2MI19niAIHi7fTnckyMdqbxv6kEPq9VlGmSMAYyRHztLd_ng)

Let’s say ada sebuah workflow seperti diatas. Task 1 mengumpulkan data data dari berbagai sumber misalnya dari S3, hasil crawling, dan sebagainya. Task 2 memasukan data tersebut kedalam HDFS. Kemudian di Task 3 memproses ETL data dari HDFS tersebut. Lalu hasil dari ETL tersebut masuk kedalam Data Warehouse dan dibuat Data Report di Task 4 dan Task 4_1.

**Learn more:** 

[Concepts - Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html)

# Mind Map

Untuk mempermudah kalian dalam belajar, berikut mind-map yang bisa kalian gunakan sebagai panduan dalam mempelajari Apache Airflow.

![Airflow Mind Map.jpg](Apache%20Airflow%20Introduction%2039f4a23c4a16449ba74c23c1083c421b/Airflow_Mind_Map.jpg)